{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7070356,"sourceType":"datasetVersion","datasetId":4071570}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom tqdm import tqdm\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.feature_selection import RFE, SelectKBest, mutual_info_regression, SelectFromModel\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nimport shap\nfrom IPython.display import display, HTML\nimport random\nimport operator\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom deap import algorithms\nfrom deap import base\nfrom deap import creator\nfrom deap import tools\nfrom deap import gp\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\npd.set_option(\"display.max_rows\", 50, \"display.max_columns\", 50)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:48:28.459878Z","iopub.execute_input":"2023-12-05T18:48:28.460263Z","iopub.status.idle":"2023-12-05T18:48:37.416187Z","shell.execute_reply.started":"2023-12-05T18:48:28.460231Z","shell.execute_reply":"2023-12-05T18:48:37.414261Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explanation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Explanation, Cohorts\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Explainer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel \u001b[38;5;28;01mas\u001b[39;00m KernelExplainer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sampling \u001b[38;5;28;01mas\u001b[39;00m SamplingExplainer\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Deep\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Exact\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_gpu_tree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPUTree\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_linear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_partition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Partition\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_gpu_tree.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_import, record_import_error\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tree, feature_perturbation_codes, output_transform_codes\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cext_gpu\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/explainers/_tree.py:20\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_import, record_import_error, safe_isinstance\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     ExplainerError,\n\u001b[1;32m     16\u001b[0m     InvalidFeaturePerturbationError,\n\u001b[1;32m     17\u001b[0m     InvalidMaskerError,\n\u001b[1;32m     18\u001b[0m     InvalidModelError,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_legacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseData\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Explainer\n\u001b[1;32m     23\u001b[0m warnings\u001b[38;5;241m.\u001b[39mformatwarning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m msg, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mstr\u001b[39m(msg) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# ignore everything except the message\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/shap/utils/_legacy.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkmeans\u001b[39m(X, k, round_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/cluster/__init__.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_spectral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spectral_clustering, SpectralClustering\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mean_shift\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_shift, MeanShift, estimate_bandwidth, get_bin_seeds\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_affinity_propagation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m affinity_propagation, AffinityPropagation\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_agglomerative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ward_tree,\n\u001b[1;32m     11\u001b[0m     AgglomerativeClustering,\n\u001b[1;32m     12\u001b[0m     linkage_tree,\n\u001b[1;32m     13\u001b[0m     FeatureAgglomeration,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kmeans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m k_means, KMeans, MiniBatchKMeans, kmeans_plusplus\n","File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:964\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:417\u001b[0m, in \u001b[0;36mcache_from_source\u001b[0;34m(path, debug_override, optimization)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"## Stats Util Functions","metadata":{}},{"cell_type":"code","source":"\ndef normalized_rmse(y_true, y_pred):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    range_y = np.max(y_true) - np.min(y_true)\n    return rmse / (range_y + 1e-8)\n\n\ndef normalized_mae(y_true, y_pred):\n    mae = mean_absolute_error(y_true, y_pred)\n    range_y = np.max(y_true) - np.min(y_true)\n    return mae / (range_y + 1e-8)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:48:37.417232Z","iopub.status.idle":"2023-12-05T18:48:37.417712Z","shell.execute_reply.started":"2023-12-05T18:48:37.417505Z","shell.execute_reply":"2023-12-05T18:48:37.417527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decide to use additional google search trends\n### additional google search trends are given as a sum per week, so remaining calculations and model will need to follow as such","metadata":{}},{"cell_type":"code","source":"ADDITIONAL = True","metadata":{"execution":{"iopub.status.busy":"2023-12-05T18:48:37.420454Z","iopub.status.idle":"2023-12-05T18:48:37.421507Z","shell.execute_reply.started":"2023-12-05T18:48:37.421240Z","shell.execute_reply":"2023-12-05T18:48:37.421308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataframe Setup","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('https://storage.googleapis.com/covid19-open-data/v3/location/US_GA.csv')\ndate = 'date'\ndf[date] = pd.to_datetime(df[date], format='%Y-%m-%d')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:07.524548Z","iopub.execute_input":"2023-11-30T19:07:07.525372Z","iopub.status.idle":"2023-11-30T19:07:07.805920Z","shell.execute_reply.started":"2023-11-30T19:07:07.525325Z","shell.execute_reply":"2023-11-30T19:07:07.804572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.read_csv('/kaggle/input/additional-google-search/google1.csv')\ndf3 = pd.read_csv('/kaggle/input/additional-google-search/google2.csv')\ndf4 = pd.read_csv('/kaggle/input/additional-google-search/google3.csv')\ndf_search = pd.merge(pd.merge(df2, df3, on='Week', how='inner'), df4, on='Week', how='inner')\n\ndef convert_to_numeric(value):\n    return pd.to_numeric(value.replace('<1', '0'))\n\ndf_search.iloc[:, 1:] = df_search.iloc[:, 1:].applymap(convert_to_numeric)\ncolumns_to_convert =  [\n    'Vaccine: (Georgia)',\n    'COVID-19 vaccine: (Georgia)',\n    'covid: (Georgia)',\n    'covid vaccine: (Georgia)',\n    'covid 19: (Georgia)',\n    'georgia covid vaccine: (Georgia)',\n    'georgia covid: (Georgia)_x',\n    'vaccine near me: (Georgia)',\n    'covid 19 georgia: (Georgia)',\n    'ga covid 19: (Georgia)',\n    'vaccine covid: (Georgia)',\n    'covid ga: (Georgia)',\n    'georgia covid: (Georgia)_y',\n    'CVS vaccine: (Georgia)',\n    'Coronavirus disease 2019: (Georgia)'\n]\ndf_search[columns_to_convert] = df_search[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndf_search[date] = pd.to_datetime(df_search['Week'], format='%Y-%m-%d')\ndf_search.drop(columns=['Week'], inplace=True)\n\ncolumn_mapping = {\n    'Vaccine: (Georgia)': 'search_additional_Vaccine',\n    'COVID-19 vaccine: (Georgia)': 'search_additional_COVID-19_vaccine',\n    'covid: (Georgia)': 'search_additional_covid',\n    'covid vaccine: (Georgia)': 'search_additional_covid_vaccine',\n    'covid 19: (Georgia)': 'search_additional_covid_19',\n    'georgia covid vaccine: (Georgia)': 'search_additional_georgia_covid_vaccine',\n    'georgia covid: (Georgia)_x': 'search_additional_georgia_covid_x',\n    'vaccine near me: (Georgia)': 'search_additional_vaccine_near_me',\n    'covid 19 georgia: (Georgia)': 'search_additional_covid_19_georgia',\n    'ga covid 19: (Georgia)': 'search_additional_ga_covid_19',\n    'vaccine covid: (Georgia)': 'search_additional_vaccine_covid',\n    'covid ga: (Georgia)': 'search_additional_covid_ga',\n    'georgia covid: (Georgia)_y': 'search_additional_georgia_covid_y',\n    'CVS vaccine: (Georgia)': 'search_additional_CVS_vaccine',\n    'Coronavirus disease 2019: (Georgia)': 'search_additional_Coronavirus_disease_2019',\n}\n\ndf_search.rename(columns=column_mapping, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:07.866081Z","iopub.execute_input":"2023-11-30T19:07:07.866507Z","iopub.status.idle":"2023-11-30T19:07:07.969799Z","shell.execute_reply.started":"2023-11-30T19:07:07.866471Z","shell.execute_reply":"2023-11-30T19:07:07.968786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Can choose between 2 label options","metadata":{}},{"cell_type":"code","source":"label = 'new_vaccine_doses_administered'\n# label = 'cumulative_vaccine_doses_administered'","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:08.185312Z","iopub.execute_input":"2023-11-30T19:07:08.185801Z","iopub.status.idle":"2023-11-30T19:07:08.191006Z","shell.execute_reply.started":"2023-11-30T19:07:08.185759Z","shell.execute_reply":"2023-11-30T19:07:08.189836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputation Strategy for Nulls","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:08.783089Z","iopub.execute_input":"2023-11-30T19:07:08.784031Z","iopub.status.idle":"2023-11-30T19:07:08.798547Z","shell.execute_reply.started":"2023-11-30T19:07:08.783994Z","shell.execute_reply":"2023-11-30T19:07:08.796885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ADDITIONAL:\n    print(df_search.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:09.229074Z","iopub.execute_input":"2023-11-30T19:07:09.229478Z","iopub.status.idle":"2023-11-30T19:07:09.238130Z","shell.execute_reply.started":"2023-11-30T19:07:09.229447Z","shell.execute_reply":"2023-11-30T19:07:09.236867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdh = [\n    \"new_confirmed\",\n    \"new_deceased\",\n    \"new_tested\",\n    \"cumulative_confirmed\",\n    \"cumulative_deceased\",\n    \"cumulative_tested\",\n    \"new_hospitalized_patients\",\n    \"cumulative_hospitalized_patients\",\n    \"current_hospitalized_patients\",\n    \"new_intensive_care_patients\",\n    \"cumulative_intensive_care_patients\",\n    \"current_intensive_care_patients\"\n]\n\ncdh_age = [\n    \"new_confirmed_age_0\",\n    \"new_confirmed_age_1\",\n    \"new_confirmed_age_2\",\n    \"new_confirmed_age_3\",\n    \"new_confirmed_age_4\",\n    \"new_confirmed_age_5\",\n    \"new_confirmed_age_6\",\n    \"new_confirmed_age_7\",\n    \"new_confirmed_age_8\",\n    \"new_confirmed_age_9\",\n    \"cumulative_confirmed_age_0\",\n    \"cumulative_confirmed_age_1\",\n    \"cumulative_confirmed_age_2\",\n    \"cumulative_confirmed_age_3\",\n    \"cumulative_confirmed_age_4\",\n    \"cumulative_confirmed_age_5\",\n    \"cumulative_confirmed_age_6\",\n    \"cumulative_confirmed_age_7\",\n    \"cumulative_confirmed_age_8\",\n    \"cumulative_confirmed_age_9\",\n    \"new_deceased_age_0\",\n    \"new_deceased_age_1\",\n    \"new_deceased_age_2\",\n    \"new_deceased_age_3\",\n    \"new_deceased_age_4\",\n    \"new_deceased_age_5\",\n    \"new_deceased_age_6\",\n    \"new_deceased_age_7\",\n    \"new_deceased_age_8\",\n    \"new_deceased_age_9\",\n    \"cumulative_deceased_age_0\",\n    \"cumulative_deceased_age_1\",\n    \"cumulative_deceased_age_2\",\n    \"cumulative_deceased_age_3\",\n    \"cumulative_deceased_age_4\",\n    \"cumulative_deceased_age_5\",\n    \"cumulative_deceased_age_6\",\n    \"cumulative_deceased_age_7\",\n    \"cumulative_deceased_age_8\",\n    \"cumulative_deceased_age_9\",\n    \"new_hospitalized_patients_age_0\",\n    \"new_hospitalized_patients_age_1\",\n    \"new_hospitalized_patients_age_2\",\n    \"new_hospitalized_patients_age_3\",\n    \"new_hospitalized_patients_age_4\",\n    \"new_hospitalized_patients_age_5\",\n    \"new_hospitalized_patients_age_6\",\n    \"new_hospitalized_patients_age_7\",\n    \"new_hospitalized_patients_age_8\",\n    \"new_hospitalized_patients_age_9\",\n    \"cumulative_hospitalized_patients_age_0\",\n    \"cumulative_hospitalized_patients_age_1\",\n    \"cumulative_hospitalized_patients_age_2\",\n    \"cumulative_hospitalized_patients_age_3\",\n    \"cumulative_hospitalized_patients_age_4\",\n    \"cumulative_hospitalized_patients_age_5\",\n    \"cumulative_hospitalized_patients_age_6\",\n    \"cumulative_hospitalized_patients_age_7\",\n    \"cumulative_hospitalized_patients_age_8\",\n    \"cumulative_hospitalized_patients_age_9\"\n]\npop = [\n    'population',\n    'population_male',\n    'population_female',\n    'population_age_00_09',\n    'population_age_10_19',\n    'population_age_20_29',\n    'population_age_30_39',\n    'population_age_40_49',\n    'population_age_50_59',\n    'population_age_60_69',\n    'population_age_70_79',\n    'population_age_80_and_older'\n]\n\nmob = [\n    'mobility_retail_and_recreation',\n    'mobility_grocery_and_pharmacy',\n    'mobility_parks',\n    'mobility_transit_stations',\n    'mobility_workplaces',\n    'mobility_residential'\n]\n\nmisc = [\n    'school_closing',\n    'workplace_closing',\n    'cancel_public_events',\n    'restrictions_on_gatherings',\n    'public_transport_closing',\n    'stay_at_home_requirements',\n    'restrictions_on_internal_movement',\n    'international_travel_controls',\n    'income_support',\n    'debt_relief',\n    'fiscal_measures',\n    'international_support',\n    'public_information_campaigns',\n    'testing_policy',\n    'contact_tracing',\n    'emergency_investment_in_healthcare',\n    'investment_in_vaccines',\n    'facial_coverings',\n    'vaccination_policy',\n    'stringency_index'\n]\nsearch = [col for col in df.columns if col.startswith(\"search_trends\")]\nweather = [\n    'average_temperature_celsius',\n    'minimum_temperature_celsius',\n    'maximum_temperature_celsius',\n    'rainfall_mm',\n    'dew_point',\n    'relative_humidity'\n]\n\nsearch_additional = [\n    'search_additional_Vaccine',\n    'search_additional_COVID-19_vaccine',\n    'search_additional_covid',\n    'search_additional_covid_vaccine',\n    'search_additional_covid_19',\n    'search_additional_georgia_covid_vaccine',\n    'search_additional_georgia_covid_x',\n    'search_additional_vaccine_near_me',\n    'search_additional_covid_19_georgia',\n    'search_additional_ga_covid_19',\n    'search_additional_vaccine_covid',\n    'search_additional_covid_ga',\n    'search_additional_georgia_covid_y',\n    'search_additional_CVS_vaccine',\n    'search_additional_Coronavirus_disease_2019',\n]","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:09.622793Z","iopub.execute_input":"2023-11-30T19:07:09.623208Z","iopub.status.idle":"2023-11-30T19:07:09.635573Z","shell.execute_reply.started":"2023-11-30T19:07:09.623175Z","shell.execute_reply":"2023-11-30T19:07:09.634321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"column_cats = [cdh, cdh_age, pop, mob, misc, search, weather]\n    \nfor category in column_cats:\n    for column in category:\n        prev_value = None\n        for index, value in df[column].items():\n            if pd.notna(value):\n                prev_value = value\n            else:\n                if prev_value is not None:\n                    df.at[index, column] = prev_value\n                else:\n                    df.at[index, column] = 0","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:10.000080Z","iopub.execute_input":"2023-11-30T19:07:10.001205Z","iopub.status.idle":"2023-11-30T19:07:11.178186Z","shell.execute_reply.started":"2023-11-30T19:07:10.001165Z","shell.execute_reply":"2023-11-30T19:07:11.176908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting Features","metadata":{}},{"cell_type":"code","source":"features = cdh + cdh_age + pop + mob + misc + search + weather\nif ADDITIONAL:\n    features += search_additional","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:11.325253Z","iopub.execute_input":"2023-11-30T19:07:11.326145Z","iopub.status.idle":"2023-11-30T19:07:11.332722Z","shell.execute_reply.started":"2023-11-30T19:07:11.326100Z","shell.execute_reply":"2023-11-30T19:07:11.330819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## If ADDITIONAL = TRUE\n### we need to sum over the last 7 days from our original df\n### we need to combine df and df_search","metadata":{}},{"cell_type":"code","source":"if ADDITIONAL:\n    numerical_columns = df.select_dtypes(include='number').columns\n    non_numerical_columns = list(df.select_dtypes(exclude='number').columns) +[label]\n\n    df_numerical = df[numerical_columns]\n    df_non_numerical = df[non_numerical_columns]\n\n    df_rolling_sum = df[cdh + cdh_age + pop + mob + misc + search + weather].rolling(window=7, min_periods=1).sum()\n\n    df_rolling_sum = df_rolling_sum.reset_index( drop=True)\n\n    df = pd.concat([df_non_numerical, df_rolling_sum], axis=1)\n    \n    df = pd.merge(df, df_search, on=date, how='inner')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:12.376131Z","iopub.execute_input":"2023-11-30T19:07:12.376562Z","iopub.status.idle":"2023-11-30T19:07:12.460836Z","shell.execute_reply.started":"2023-11-30T19:07:12.376525Z","shell.execute_reply":"2023-11-30T19:07:12.459685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Determing Date Range \n### Vaccinations were not available for a while, so lets look at null ranges","metadata":{}},{"cell_type":"code","source":"data_subset = df[[date,label]]\n\nnull_count_by_date = data_subset.groupby(date).apply(lambda x: x[label].isnull().sum())\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=null_count_by_date)\nplt.xlabel('Date')\nplt.ylabel('Count of Null Values')\nplt.title('Null Values in Vaccination Count by Date')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:13.551151Z","iopub.execute_input":"2023-11-30T19:07:13.551549Z","iopub.status.idle":"2023-11-30T19:07:14.105559Z","shell.execute_reply.started":"2023-11-30T19:07:13.551519Z","shell.execute_reply":"2023-11-30T19:07:14.103834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlongest_continuous_range_start = None\nlongest_continuous_range_end = None\n\ncurrent_continuous_range_start = None\ncurrent_continuous_range_end = None\n\nfor index, value in df[label].items():\n    if pd.notna(value):\n        if current_continuous_range_start is None:\n            current_continuous_range_start = index\n        current_continuous_range_end = index\n    else:\n        if (current_continuous_range_start is not None) and (current_continuous_range_end is not None):\n            if (longest_continuous_range_start is None) or (current_continuous_range_end - current_continuous_range_start > longest_continuous_range_end - longest_continuous_range_start):\n                longest_continuous_range_start = current_continuous_range_start\n                longest_continuous_range_end = current_continuous_range_end\n        current_continuous_range_start = None\n        current_continuous_range_end = None\n\nprint(f\"Start: Index {longest_continuous_range_start}\")\nprint(f\"End: Index {longest_continuous_range_end}\")\n\nstart_date = df.at[longest_continuous_range_start, 'date']\nend_date = df.at[longest_continuous_range_end, 'date']\n\nprint(f\"Start Date: {start_date}\")\nprint(f\"End Date: {end_date}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:14.107398Z","iopub.execute_input":"2023-11-30T19:07:14.107846Z","iopub.status.idle":"2023-11-30T19:07:14.118398Z","shell.execute_reply.started":"2023-11-30T19:07:14.107806Z","shell.execute_reply":"2023-11-30T19:07:14.117246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[longest_continuous_range_start:longest_continuous_range_end].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:14.997034Z","iopub.execute_input":"2023-11-30T19:07:14.998167Z","iopub.status.idle":"2023-11-30T19:07:15.003694Z","shell.execute_reply.started":"2023-11-30T19:07:14.998127Z","shell.execute_reply":"2023-11-30T19:07:15.002853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vaccination Uptake Delay","metadata":{}},{"cell_type":"code","source":"date_column = 'date'\nvaccine_column = label\n\n\nplt.figure(figsize=(12, 6))\nplt.plot(df[date_column], df[vaccine_column], label='Actual Vaccination Values', color='blue')\nplt.xlabel('Date')\nplt.ylabel('Vaccination Values')\nplt.title('Actual Vaccination Values Over Time')\nplt.legend()\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:20.866054Z","iopub.execute_input":"2023-11-30T19:07:20.866819Z","iopub.status.idle":"2023-11-30T19:07:21.403131Z","shell.execute_reply.started":"2023-11-30T19:07:20.866784Z","shell.execute_reply":"2023-11-30T19:07:21.401829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## When ADDITIONAL = FALSE\n### Note the spikes occuring at 7, 14, 21, 28, incidicating some week like structure\n## When ADDITIONAL = TRUE\n### No obvious correlations","metadata":{}},{"cell_type":"code","source":"acf_result = plot_acf(df[vaccine_column], lags=30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:22.485397Z","iopub.execute_input":"2023-11-30T19:07:22.486630Z","iopub.status.idle":"2023-11-30T19:07:22.775354Z","shell.execute_reply.started":"2023-11-30T19:07:22.486589Z","shell.execute_reply":"2023-11-30T19:07:22.774344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## When ADDITIONAL = FALSE:\n### No obvious date to use, so we can test with all of them\n### AKA: weekly sum on monday, tuesday, wednesday ... and see which performs the best\n### Will be done after dealing with features","metadata":{}},{"cell_type":"code","source":"if not ADDITIONAL:\n    \n    df['day_of_week'] = df['date'].dt.day_name()\n\n    day_counts = df.groupby('day_of_week')[label].sum()\n\n    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    day_counts = day_counts.reindex(days_of_week)\n\n    plt.figure(figsize=(10, 6))\n    day_counts.plot(kind='bar', color='skyblue')\n    plt.title('Total Vaccinations by Day of the Week')\n    plt.xlabel('Day of the Week')\n    plt.ylabel('Total Number of Vaccinations')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:25.507348Z","iopub.execute_input":"2023-11-30T19:07:25.507831Z","iopub.status.idle":"2023-11-30T19:07:25.515305Z","shell.execute_reply.started":"2023-11-30T19:07:25.507793Z","shell.execute_reply":"2023-11-30T19:07:25.514066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"leads = []\nfor feature in features:\n    cross_corr = np.correlate(df[feature].values, df[label].values, mode='full')\n    time_lags = np.arange(-len(df[feature])+1, len(df[feature]))\n    \n        # Find the lag with maximum correlation\n    max_corr_index = np.argmax(cross_corr)\n    max_corr_lag = time_lags[max_corr_index]\n    \n    if max_corr_lag <= -1 and 'search_additional' in feature:\n\n        # Plot cross-correlation\n        plt.figure(figsize=(10, 5))\n        plt.plot(time_lags, cross_corr, marker='o', linestyle='-')\n        plt.title(f'Cross-Correlation between {feature} and Label')\n        plt.xlabel('Time Lag')\n        plt.ylabel('Cross-Correlation')\n        plt.grid(True)\n        plt.show()\n\n    \n    print(f\"Optimal lag for {feature}: {max_corr_lag}\")\n    \n    if max_corr_lag < 0:\n        leads.append((feature, max_corr_lag))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:27.878159Z","iopub.execute_input":"2023-11-30T19:07:27.878596Z","iopub.status.idle":"2023-11-30T19:07:30.529175Z","shell.execute_reply.started":"2023-11-30T19:07:27.878561Z","shell.execute_reply":"2023-11-30T19:07:30.527917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leads","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:30.531129Z","iopub.execute_input":"2023-11-30T19:07:30.531484Z","iopub.status.idle":"2023-11-30T19:07:30.543152Z","shell.execute_reply.started":"2023-11-30T19:07:30.531453Z","shell.execute_reply":"2023-11-30T19:07:30.542107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Storing old df without lag features and creating lagged df","metadata":{}},{"cell_type":"code","source":"# df = df_og.copy()\ndf_og = df.copy()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:31.546904Z","iopub.execute_input":"2023-11-30T19:07:31.547325Z","iopub.status.idle":"2023-11-30T19:07:31.554007Z","shell.execute_reply.started":"2023-11-30T19:07:31.547293Z","shell.execute_reply":"2023-11-30T19:07:31.552475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"limit = -60\nif ADDITIONAL:\n    limit = -4\nfor feature, lag in leads:\n    if lag >= limit:\n        df[feature] = df[feature].shift(lag)\n    \ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:32.330084Z","iopub.execute_input":"2023-11-30T19:07:32.330512Z","iopub.status.idle":"2023-11-30T19:07:32.373511Z","shell.execute_reply.started":"2023-11-30T19:07:32.330475Z","shell.execute_reply":"2023-11-30T19:07:32.372140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:43.095198Z","iopub.execute_input":"2023-11-30T19:07:43.096483Z","iopub.status.idle":"2023-11-30T19:07:43.103784Z","shell.execute_reply.started":"2023-11-30T19:07:43.096439Z","shell.execute_reply":"2023-11-30T19:07:43.102583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for group, columns in zip([\"cdh\", \"cdh_age\", \"pop\", \"mob\", \"misc\", \"search\", \"search_additional\",\"weather\"],\n                         [cdh, cdh_age, pop, mob, misc, search, search_additional, weather]):\n    print(f\"Group: {group}\")\n    \n    summary = df[columns].describe()\n    print(\"Summary Statistics:\")\n    print(summary)\n    \n    for column in columns:\n        plt.figure(figsize=(10, 4))\n        sns.histplot(df[column], kde=True)\n        plt.title(f'Distribution of {column}')\n        plt.xlabel(column)\n        plt.ylabel('Frequency')\n        plt.show()\n    \n    correlation_matrix = df[columns].corr()\n    print(\"Correlation Matrix:\")\n    print(correlation_matrix)\n    \n    plt.figure(figsize=(10, 6))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n    plt.title(f'Correlation Matrix: {group}')\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:07:32.812086Z","iopub.execute_input":"2023-11-30T19:07:32.812514Z","iopub.status.idle":"2023-11-30T19:07:38.490271Z","shell.execute_reply.started":"2023-11-30T19:07:32.812479Z","shell.execute_reply":"2023-11-30T19:07:38.487868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"groups = [cdh, cdh_age, pop, mob, misc, search, weather]\nfor group in groups:\n    print(f\"Group: {str(group)}\")\n    \n    # Pearson Correlation\n    pearson_correlations = {}\n    for feature in group:\n        if feature != label:\n            correlation, _ = pearsonr(df[feature], df[label])\n            pearson_correlations[feature] = correlation\n    \n    sorted_pearson_correlations = {k: v for k, v in sorted(pearson_correlations.items(), key=lambda item: abs(item[1]), reverse=True)}\n    \n    top_pearson_features = list(sorted_pearson_correlations.keys())[:10]\n    top_pearson_values = list(sorted_pearson_correlations.values())[:10]\n    \n    plt.figure(figsize=(12, 4))\n    plt.barh(top_pearson_features, top_pearson_values)\n    plt.title(f'Top 10 Features by Pearson Correlation with {label}')\n    plt.xlabel('Correlation')\n    plt.show()\n    \n    # Spearman Correlation\n    spearman_correlations = {}\n    for feature in group:\n        if feature != label:\n            correlation, _ = spearmanr(df[feature], df[label])\n            spearman_correlations[feature] = correlation\n    \n    sorted_spearman_correlations = {k: v for k, v in sorted(spearman_correlations.items(), key=lambda item: abs(item[1]), reverse=True)}\n    \n    top_spearman_features = list(sorted_spearman_correlations.keys())[:10]\n    top_spearman_values = list(sorted_spearman_correlations.values())[:10]\n    \n    plt.figure(figsize=(12, 4))\n    plt.barh(top_spearman_features, top_spearman_values)\n    plt.title(f'Top 10 Features by Spearman Correlation with {label}')\n    plt.xlabel('Correlation')\n    plt.show()\n    \n    # Mutual Information\n    mi_scores = mutual_info_regression(df[group], df[label], discrete_features='auto')\n    \n    sorted_mi_scores = {k: v for k, v in sorted(enumerate(mi_scores), key=lambda item: abs(item[1]), reverse=True)}\n    \n    top_mi_features = [group[k] for k in list(sorted_mi_scores.keys())[:10]]\n    top_mi_scores = list(sorted_mi_scores.values())[:10]\n    \n    plt.figure(figsize=(12, 4))\n    plt.barh(top_mi_features, top_mi_scores)\n    plt.title(f'Top 10 Features by Mutual Information with {label}')\n    plt.xlabel('Mutual Information Score')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:10:41.875336Z","iopub.execute_input":"2023-11-30T19:10:41.875838Z","iopub.status.idle":"2023-11-30T19:10:41.883296Z","shell.execute_reply.started":"2023-11-30T19:10:41.875799Z","shell.execute_reply":"2023-11-30T19:10:41.881722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=5)\n\nscaler = StandardScaler()\n\ndef get_top_k_features(df, k):\n    top_k_features = []\n\n    for train_index, test_index in tscv.split(df):\n        train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n        X_train, X_test = train_data[features], test_data[features]\n        y_train, y_test = train_data[label], test_data[label]\n\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n\n        rfe_selector = RFE(estimator=RandomForestRegressor(n_estimators=10), n_features_to_select=k)\n        rfe_selector.fit(X_train_scaled, y_train)\n        rfe_support = rfe_selector.support_\n\n        skb_selector = SelectKBest(mutual_info_regression, k=k)\n        skb_selector.fit(X_train_scaled, y_train)\n        skb_support = skb_selector.get_support()\n\n        lasso_selector = SelectFromModel(Lasso(alpha=0.01))\n        lasso_selector.fit(X_train_scaled, y_train)\n        lasso_support = lasso_selector.get_support()\n\n        gb_selector = SelectFromModel(GradientBoostingRegressor(n_estimators=100))\n        gb_selector.fit(X_train_scaled, y_train)\n        gb_support = gb_selector.get_support()\n\n        feature_support = {\n            'RFE': rfe_support,\n            'SelectKBest': skb_support,\n            'LASSO': lasso_support,\n            'GradientBoosting': gb_support,\n        }\n\n        votes = np.sum([feature_support[method] for method in feature_support.keys()], axis=0)\n        top_k_features.extend(np.argsort(votes)[-k:])\n\n    top_k_features = np.unique(top_k_features)[:k]\n\n    return top_k_features\n\nk_features_to_select = 50\ntop_k_features = get_top_k_features(df, k_features_to_select)\n\nselected_feature_names = df.columns[top_k_features]\nprint(f\"Top {k_features_to_select} Features: {selected_feature_names}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:10:42.861442Z","iopub.execute_input":"2023-11-30T19:10:42.862657Z","iopub.status.idle":"2023-11-30T19:10:42.870473Z","shell.execute_reply.started":"2023-11-30T19:10:42.862580Z","shell.execute_reply":"2023-11-30T19:10:42.869249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Can Test Mutiple feature selections below","metadata":{}},{"cell_type":"code","source":"# features = list(selected_feature_names)\n# # features = cdh + cdh_age + pop + mob + misc + search + weather\n# features = search\n# if ADDITIONAL:\n#     features += search_additional","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:10:43.047617Z","iopub.execute_input":"2023-11-30T19:10:43.048067Z","iopub.status.idle":"2023-11-30T19:10:43.053143Z","shell.execute_reply.started":"2023-11-30T19:10:43.048027Z","shell.execute_reply":"2023-11-30T19:10:43.051904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling without Rolling Sums per week\n## ONLY RUNS IF ADDITIONAL = FALSE","metadata":{"execution":{"iopub.status.busy":"2023-11-28T18:07:41.299860Z","iopub.execute_input":"2023-11-28T18:07:41.300311Z","iopub.status.idle":"2023-11-28T18:07:41.305578Z","shell.execute_reply.started":"2023-11-28T18:07:41.300275Z","shell.execute_reply":"2023-11-28T18:07:41.304357Z"}}},{"cell_type":"code","source":"if not ADDITIONAL:\n    \n    tscv = TimeSeriesSplit(n_splits=10)\n\n    for train_index, test_index in tscv.split(df):\n        train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n        X_train, X_test = train_data[features], test_data[features]\n        y_train, y_test = train_data[label], test_data[label]\n\n        rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n        rf_regressor.fit(X_train, y_train)\n        rf_predictions = rf_regressor.predict(X_test)\n\n        mse_rf = mean_squared_error(y_test, rf_predictions)\n        mae_rf = mean_absolute_error(y_test, rf_predictions)\n        nmae_rf = normalized_mae(y_test, rf_predictions)\n        nrmse_rf = normalized_rmse(y_test, rf_predictions)\n        r2_rf = r2_score(y_test, rf_predictions)\n\n        xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n        xgb_regressor.fit(X_train, y_train)\n        xgb_predictions = xgb_regressor.predict(X_test)\n\n        mse_xgb = mean_squared_error(y_test, xgb_predictions)\n        mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n        nmae_xgb = normalized_mae(y_test, xgb_predictions)\n        nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n        r2_xgb = r2_score(y_test, xgb_predictions)\n\n        ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n        ada_regressor.fit(X_train, y_train)\n        ada_predictions = ada_regressor.predict(X_test)\n\n        mse_ada = mean_squared_error(y_test, ada_predictions)\n        mae_ada = mean_absolute_error(y_test, ada_predictions)\n        nmae_ada = normalized_mae(y_test, ada_predictions)\n        nrmse_ada = normalized_rmse(y_test, ada_predictions)\n        r2_ada = r2_score(y_test, ada_predictions)\n\n        print(\"Random Forest Metrics:\")\n        print(f\"Mean Squared Error: {mse_rf}\")\n        print(f\"Mean Absolute Error: {mae_rf}\")\n        print(f\"Normalized RMSE: {nrmse_rf}\")\n        print(f\"Normalized MAE: {nmae_rf}\")\n        print(f\"R-squared: {r2_rf}\")\n\n        print(\"\\nXGBoost Metrics:\")\n        print(f\"Mean Squared Error: {mse_xgb}\")\n        print(f\"Mean Absolute Error: {mae_xgb}\")\n        print(f\"Normalized RMSE: {nrmse_xgb}\")\n        print(f\"Normalized MAE: {nmae_xgb}\")\n        print(f\"R-squared: {r2_xgb}\")\n\n        print(\"\\nAdaBoost Metrics:\")\n        print(f\"Mean Squared Error: {mse_ada}\")\n        print(f\"Mean Absolute Error: {mae_ada}\")\n        print(f\"Normalized RMSE: {nrmse_ada}\")\n        print(f\"Normalized MAE: {nmae_ada}\")\n        print(f\"R-squared: {r2_ada}\")\n\n        # Plot the regression curves\n        plt.figure(figsize=(10, 6))\n        plt.plot(test_data['date'], y_test, label='Actual', color='blue')\n        plt.plot(test_data['date'], rf_predictions, label='Random Forest', color='red')\n        plt.plot(test_data['date'], xgb_predictions, label='XGBoost', color='green')\n        plt.plot(test_data['date'], ada_predictions, label='AdaBoost', color='purple')\n        plt.xlabel('Date')\n        plt.ylabel('Label')\n        plt.legend()\n        plt.title('Regression Curves')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:10:44.197574Z","iopub.execute_input":"2023-11-30T19:10:44.198036Z","iopub.status.idle":"2023-11-30T19:10:44.218916Z","shell.execute_reply.started":"2023-11-30T19:10:44.198002Z","shell.execute_reply":"2023-11-30T19:10:44.216961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling with Rolling Sums per week\n## If ADDITIONAL = TRUE\n### Rolling is already applied and only weekly values are used\n## If ADDITIONAL = FALSE -> Two methods: \n### 1. rolling sum per 7 days\n### 2. sum per week as index (weekly values)","metadata":{}},{"cell_type":"code","source":"if ADDITIONAL:\n    df_weekly_sum = df\nelse: \n    numerical_columns = df.select_dtypes(include='number').columns\n    non_numerical_columns = df.select_dtypes(exclude='number').columns\n\n    df_numerical = df[numerical_columns]\n    df_non_numerical = df[non_numerical_columns]\n\n    df_rolling_sum = df[features + [label] + ['day_of_week']].groupby('day_of_week').rolling(window=7, min_periods=1).sum()\n\n    df_rolling_sum = df_rolling_sum.reset_index( drop=True)\n\n    df_weekly_sum = pd.concat([df_non_numerical, df_rolling_sum], axis=1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(df_weekly_sum.index, df_weekly_sum[label], marker='o', linestyle='-', color='b')\nplt.title('Weekly Sum of Vaccinations')\nplt.xlabel('Date')\nplt.ylabel('Weekly Sum of Vaccinations')\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:10:52.267903Z","iopub.execute_input":"2023-11-30T19:10:52.268306Z","iopub.status.idle":"2023-11-30T19:10:52.593710Z","shell.execute_reply.started":"2023-11-30T19:10:52.268274Z","shell.execute_reply":"2023-11-30T19:10:52.592319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method 1: \n### If ADDITIONAL = TRUE: Modeling with rolling sum values per week (every week is a datapoint)\n### Else: Modeling with all rolling sum values (every day is a datapoint)","metadata":{}},{"cell_type":"code","source":"tscv = TimeSeriesSplit(n_splits=5)\n\ndf_weekly_sum.reset_index(drop=True, inplace=True)\n\ncumulative_metrics_rf = {'MSE': 0, 'MAE': 0, 'NMAE': 0, 'NRMSE': 0, 'R2': 0}\ncumulative_metrics_xgb = {'MSE': 0, 'MAE': 0, 'NMAE': 0, 'NRMSE': 0, 'R2': 0}\ncumulative_metrics_ada = {'MSE': 0, 'MAE': 0, 'NMAE': 0, 'NRMSE': 0, 'R2': 0}\n\ncumulative_predictions_rf = []\ncumulative_predictions_xgb = []\ncumulative_predictions_ada = []\n\nfor train_index, test_index in tscv.split(df_weekly_sum):\n    print(train_index, test_index)\n    train_data, test_data = df_weekly_sum.iloc[train_index], df_weekly_sum.iloc[test_index]\n    X_train, X_test = train_data[features], test_data[features]\n    y_train, y_test = train_data[label], test_data[label]\n\n    # Random Forest\n    rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n    rf_regressor.fit(X_train, y_train)\n    rf_predictions = rf_regressor.predict(X_test)\n\n    # XGBoost\n    xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n    xgb_regressor.fit(X_train, y_train)\n    xgb_predictions = xgb_regressor.predict(X_test)\n\n    # AdaBoost\n    ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n    ada_regressor.fit(X_train, y_train)\n    ada_predictions = ada_regressor.predict(X_test)\n\n    # Append predictions to cumulative arrays\n    cumulative_predictions_rf.extend(rf_predictions)\n    cumulative_predictions_xgb.extend(xgb_predictions)\n    cumulative_predictions_ada.extend(ada_predictions)\n\n    # Calculate metrics for Random Forest\n    mse_rf = mean_squared_error(y_test, rf_predictions)\n    mae_rf = mean_absolute_error(y_test, rf_predictions)\n    nmae_rf = normalized_mae(y_test, rf_predictions)\n    nrmse_rf = normalized_rmse(y_test, rf_predictions)\n    r2_rf = r2_score(y_test, rf_predictions)\n\n    # Update cumulative metrics for Random Forest\n    cumulative_metrics_rf['MSE'] += mse_rf\n    cumulative_metrics_rf['MAE'] += mae_rf\n    cumulative_metrics_rf['NMAE'] += nmae_rf\n    cumulative_metrics_rf['NRMSE'] += nrmse_rf\n    cumulative_metrics_rf['R2'] += r2_rf\n\n    # Calculate metrics for XGBoost\n    mse_xgb = mean_squared_error(y_test, xgb_predictions)\n    mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n    nmae_xgb = normalized_mae(y_test, xgb_predictions)\n    nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n    r2_xgb = r2_score(y_test, xgb_predictions)\n\n    # Update cumulative metrics for XGBoost\n    cumulative_metrics_xgb['MSE'] += mse_xgb\n    cumulative_metrics_xgb['MAE'] += mae_xgb\n    cumulative_metrics_xgb['NMAE'] += nmae_xgb\n    cumulative_metrics_xgb['NRMSE'] += nrmse_xgb\n    cumulative_metrics_xgb['R2'] += r2_xgb\n\n    # Calculate metrics for AdaBoost\n    mse_ada = mean_squared_error(y_test, ada_predictions)\n    mae_ada = mean_absolute_error(y_test, ada_predictions)\n    nmae_ada = normalized_mae(y_test, ada_predictions)\n    nrmse_ada = normalized_rmse(y_test, ada_predictions)\n    r2_ada = r2_score(y_test, ada_predictions)\n\n    # Update cumulative metrics for AdaBoost\n    cumulative_metrics_ada['MSE'] += mse_ada\n    cumulative_metrics_ada['MAE'] += mae_ada\n    cumulative_metrics_ada['NMAE'] += nmae_ada\n    cumulative_metrics_ada['NRMSE'] += nrmse_ada\n    cumulative_metrics_ada['R2'] += r2_ada\n\n    # Plot regression curves for each time split\n    plt.figure(figsize=(10, 6))\n    plt.plot(test_data.index, y_test, label='Actual', color='blue')\n    plt.plot(test_data.index, rf_predictions, label='Random Forest', color='red')\n    plt.plot(test_data.index, xgb_predictions, label='XGBoost', color='green')\n    plt.plot(test_data.index, ada_predictions, label='AdaBoost', color='purple')\n    plt.xlabel('Index')\n    plt.ylabel('Label')\n    plt.legend()\n    plt.title(f'Regression Curves - Time Split {train_index.max() + 1} to {test_index.max()}')\n    plt.show()\n\n# Create combined plot for overall forecast\nplt.figure(figsize=(15, 8))\nplt.plot(df_weekly_sum.iloc[15:,:].index, df_weekly_sum.iloc[15:,:][label], label='Actual', color='blue')\nplt.plot(df_weekly_sum.iloc[15:,:].index, cumulative_predictions_rf, label='Random Forest', color='red')\nplt.plot(df_weekly_sum.iloc[15:,:].index, cumulative_predictions_xgb, label='XGBoost', color='green')\nplt.plot(df_weekly_sum.iloc[15:,:].index, cumulative_predictions_ada, label='AdaBoost', color='purple')\nplt.xlabel('Index')\nplt.ylabel('Label')\nplt.legend()\nplt.title('Combined Regression Curves - Overall Forecast')\nplt.show()\n\n# Calculate average metrics over all time splits\nnum_splits = tscv.get_n_splits(df_weekly_sum)\naverage_metrics_rf = {key: value / num_splits for key, value in cumulative_metrics_rf.items()}\naverage_metrics_xgb = {key: value / num_splits for key, value in cumulative_metrics_xgb.items()}\naverage_metrics_ada = {key: value / num_splits for key, value in cumulative_metrics_ada.items()}\n\n# Print average metrics\nprint(\"\\nAverage Metrics - Random Forest:\")\nprint(average_metrics_rf)\n\nprint(\"\\nAverage Metrics - XGBoost:\")\nprint(average_metrics_xgb)\n\nprint(\"\\nAverage Metrics - AdaBoost:\")\nprint(average_metrics_ada)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T19:26:06.893080Z","iopub.execute_input":"2023-11-30T19:26:06.893491Z","iopub.status.idle":"2023-11-30T19:26:48.784876Z","shell.execute_reply.started":"2023-11-30T19:26:06.893458Z","shell.execute_reply":"2023-11-30T19:26:48.783733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code below is given if expansion training is preferred over time series split","metadata":{}},{"cell_type":"code","source":"# label = 'new_vaccine_doses_administered'\n# tscv = TimeSeriesSplit(n_splits=5)\n\n# df_weekly_sum.reset_index(drop=True, inplace=True)\n\n# num_expansions = 10\n# expansion_size = len(df_weekly_sum) // num_expansions\n\n# for expansion in range(num_expansions):\n#     train_data = df_weekly_sum.iloc[:expansion_size * (expansion + 1)]\n#     test_data = df_weekly_sum.iloc[expansion_size * (expansion + 1):expansion_size * (expansion + 2)]\n\n#     X_train, X_test = train_data[features], test_data[features]\n#     y_train, y_test = train_data[label], test_data[label]\n\n#     rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n#     rf_regressor.fit(X_train, y_train)\n#     rf_predictions = rf_regressor.predict(X_test)\n\n#     xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n#     xgb_regressor.fit(X_train, y_train)\n#     xgb_predictions = xgb_regressor.predict(X_test)\n\n#     ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n#     ada_regressor.fit(X_train, y_train)\n#     ada_predictions = ada_regressor.predict(X_test)\n\n#     mse_rf = mean_squared_error(y_test, rf_predictions)\n#     mae_rf = mean_absolute_error(y_test, rf_predictions)\n#     nmae_rf = normalized_mae(y_test, rf_predictions)\n#     nrmse_rf = normalized_rmse(y_test, rf_predictions)\n#     r2_rf = r2_score(y_test, rf_predictions)\n\n#     mse_xgb = mean_squared_error(y_test, xgb_predictions)\n#     mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n#     nmae_xgb = normalized_mae(y_test, xgb_predictions)\n#     nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n#     r2_xgb = r2_score(y_test, xgb_predictions)\n\n#     mse_ada = mean_squared_error(y_test, ada_predictions)\n#     mae_ada = mean_absolute_error(y_test, ada_predictions)\n#     nmae_ada = normalized_mae(y_test, ada_predictions)\n#     nrmse_ada = normalized_rmse(y_test, ada_predictions)\n#     r2_ada = r2_score(y_test, ada_predictions)\n\n#     print(f\"\\nExpansion {expansion + 1} Metrics:\")\n#     print(\"\\nRandom Forest Metrics:\")\n#     print(f\"Mean Squared Error: {mse_rf}\")\n#     print(f\"Mean Absolute Error: {mae_rf}\")\n#     print(f\"Normalized RMSE: {nrmse_rf}\")\n#     print(f\"Normalized MAE: {nmae_rf}\")\n#     print(f\"R-squared: {r2_rf}\")\n\n#     print(\"\\nXGBoost Metrics:\")\n#     print(f\"Mean Squared Error: {mse_xgb}\")\n#     print(f\"Mean Absolute Error: {mae_xgb}\")\n#     print(f\"Normalized RMSE: {nrmse_xgb}\")\n#     print(f\"Normalized MAE: {nmae_xgb}\")\n#     print(f\"R-squared: {r2_xgb}\")\n\n#     print(\"\\nAdaBoost Metrics:\")\n#     print(f\"Mean Squared Error: {mse_ada}\")\n#     print(f\"Mean Absolute Error: {mae_ada}\")\n#     print(f\"Normalized RMSE: {nrmse_ada}\")\n#     print(f\"Normalized MAE: {nmae_ada}\")\n#     print(f\"R-squared: {r2_ada}\")\n\n#     plt.figure(figsize=(10, 6))\n#     plt.plot(test_data.index, y_test, label='Actual', color='blue')\n#     plt.plot(test_data.index, rf_predictions, label='Random Forest', color='red')\n#     plt.plot(test_data.index, xgb_predictions, label='XGBoost', color='green')\n#     plt.plot(test_data.index, ada_predictions, label='AdaBoost', color='purple')\n#     plt.xlabel('Index')\n#     plt.ylabel('Label')\n#     plt.legend()\n#     plt.title(f'Regression Curves - Expansion {expansion + 1}')\n#     plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method 2: Using Rolling Sum for weekly data points\n### Will only run if Additional = False","metadata":{}},{"cell_type":"code","source":"if not ADDITIONAL:\n    \n    plt.figure(figsize=(10, 6))\n\n    for day in df_weekly_sum['day_of_week'].unique():\n        df_day = df[df['day_of_week'] == day]\n        plt.plot(df_day.index, df_day[label], marker='o', linestyle='-', label=day)\n\n    plt.title('Daily Vaccinations for Each Day of the Week')\n    plt.xlabel('Date')\n    plt.ylabel('Daily Vaccinations')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if  not ADDITIONAL:\n\n    tscv = TimeSeriesSplit(n_splits=5)\n\n    results_df = pd.DataFrame(columns=['Day', 'Model', 'MSE', 'MAE', 'NMAE', 'NRMSE', 'R2'])\n\n    for day in df_weekly_sum['day_of_week'].unique():\n\n        df_day = df_weekly_sum[df_weekly_sum['day_of_week'] == day]\n\n        for train_index, test_index in tscv.split(df_day):\n            train_data, test_data = df_day.iloc[train_index], df_day.iloc[test_index]\n            X_train, X_test = train_data[features], test_data[features]\n            y_train, y_test = train_data[label], test_data[label]\n\n            rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n            rf_regressor.fit(X_train, y_train)\n            rf_predictions = rf_regressor.predict(X_test)\n\n            xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n            xgb_regressor.fit(X_train, y_train)\n            xgb_predictions = xgb_regressor.predict(X_test)\n\n            ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n            ada_regressor.fit(X_train, y_train)\n            ada_predictions = ada_regressor.predict(X_test)\n\n            mse_rf = mean_squared_error(y_test, rf_predictions)\n            mae_rf = mean_absolute_error(y_test, rf_predictions)\n            nmae_rf = normalized_mae(y_test, rf_predictions)\n            nrmse_rf = normalized_rmse(y_test, rf_predictions)\n            r2_rf = r2_score(y_test, rf_predictions)\n\n            mse_xgb = mean_squared_error(y_test, xgb_predictions)\n            mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n            nmae_xgb = normalized_mae(y_test, xgb_predictions)\n            nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n            r2_xgb = r2_score(y_test, xgb_predictions)\n\n            mse_ada = mean_squared_error(y_test, ada_predictions)\n            mae_ada = mean_absolute_error(y_test, ada_predictions)\n            nmae_ada = normalized_mae(y_test, ada_predictions)\n            nrmse_ada = normalized_rmse(y_test, ada_predictions)\n            r2_ada = r2_score(y_test, ada_predictions)\n\n            results_df = pd.concat([results_df, pd.DataFrame({\n                'Day': [day],\n                'Model': ['Random Forest'],\n                'MSE': [mse_rf],\n                'MAE': [mae_rf],\n                'NMAE': [nmae_rf],\n                'NRMSE': [nrmse_rf],\n                'R2': [r2_rf]\n            })], ignore_index=True)\n\n            results_df = pd.concat([results_df, pd.DataFrame({\n                'Day': [day],\n                'Model': ['XGBoost'],\n                'MSE': [mse_xgb],\n                'MAE': [mae_xgb],\n                'NMAE': [nmae_xgb],\n                'NRMSE': [nrmse_xgb],\n                'R2': [r2_xgb]\n            })], ignore_index=True)\n\n            results_df = pd.concat([results_df, pd.DataFrame({\n                'Day': [day],\n                'Model': ['AdaBoost'],\n                'MSE': [mse_ada],\n                'MAE': [mae_ada],\n                'NMAE': [nmae_ada],\n                'NRMSE': [nrmse_ada],\n                'R2': [r2_ada]\n            })], ignore_index=True)\n\n            # Plot the regression curves\n            plt.figure(figsize=(10, 6))\n            plt.plot(test_data['date'], y_test, label='Actual', color='blue')\n            plt.plot(test_data['date'], rf_predictions, label='Random Forest', color='red')\n            plt.plot(test_data['date'], xgb_predictions, label='XGBoost', color='green')\n            plt.plot(test_data['date'], ada_predictions, label='AdaBoost', color='purple')\n            plt.xlabel('Date')\n            plt.ylabel('Label')\n            plt.legend()\n            plt.title(f'Regression Curves for Day {day}')\n            plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not ADDITIONAL:\n    results_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not ADDITIONAL:\n    \n    for model in results_df['Model'].unique():\n        model_results = results_df[results_df['Model'] == model]\n        plt.figure(figsize=(10, 6))\n        for day in df_weekly_sum['day_of_week'].unique():\n            day_results = model_results[model_results['Day'] == day]\n            plt.plot(day_results['Day'], day_results['NMAE'], label=f'{model} - {day}')\n\n        plt.title(f'Normalized MAE for {model} by Day of Week')\n        plt.xlabel('Day of Week')\n        plt.ylabel('Normalized MAE')\n        plt.legend()\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Genetic Programming","metadata":{}},{"cell_type":"markdown","source":"## Select Label","metadata":{}},{"cell_type":"code","source":"label = 'new_vaccine_doses_administered'","metadata":{"execution":{"iopub.status.busy":"2023-11-30T15:02:20.906127Z","iopub.execute_input":"2023-11-30T15:02:20.906658Z","iopub.status.idle":"2023-11-30T15:02:20.913026Z","shell.execute_reply.started":"2023-11-30T15:02:20.906620Z","shell.execute_reply":"2023-11-30T15:02:20.911581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Toolbox\n### Primitive, Terminals, Fitness, Mutation, Mating, Population Generation, and Selection Functions","metadata":{}},{"cell_type":"code","source":"def exp_func(x):\n    return np.exp(np.clip(x, -700, 700))\n\npset = gp.PrimitiveSet(\"MAIN\", arity=len(features))\npset.addPrimitive(np.add, arity=2)\npset.addPrimitive(np.subtract, arity=2)\npset.addPrimitive(np.multiply, arity=2)\npset.addPrimitive(np.negative, arity=1)\npset.addPrimitive(np.sin, arity=1)\npset.addPrimitive(np.cos, arity=1)\n# pset.addTerminal(1.0, name=\"constant_one\")\n# pset.addTerminal(2.0, name=\"constant_two\")\n# pset.addTerminal(3.0, name=\"constant_three\")\n# pset.addPrimitive(exp_func, arity=1, name=\"exp\")\n\n\ndef evalSymbReg(individual, X_train, y_train):\n    func = gp.compile(expr=individual, pset=pset)\n    predictions = func(*X_train.T)\n    \n    mse = mean_squared_error(y_train, predictions)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_train, predictions)\n    r2 = r2_score(y_train, predictions)\n    \n    return rmse, mae, r2\n\ncreator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, -1.0, 1.0))\ncreator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMulti)\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=25)\ntoolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr)\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\ntoolbox.register(\"compile\", gp.compile, pset=pset)\ntoolbox.register(\"evaluate\", evalSymbReg)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\ntoolbox.register(\"mate\", gp.cxOnePoint)\ntoolbox.register(\"mutate\", gp.mutNodeReplacement, pset=pset)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T15:19:43.220907Z","iopub.execute_input":"2023-11-30T15:19:43.221416Z","iopub.status.idle":"2023-11-30T15:19:43.248360Z","shell.execute_reply.started":"2023-11-30T15:19:43.221369Z","shell.execute_reply":"2023-11-30T15:19:43.246233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set Parameters, Run Generations, and View Results\n### Current Values are 50 Gen and 1000 Population Size\n### DECREASE to run faster","metadata":{}},{"cell_type":"code","source":"gen = range(50)\navg_list = []\nmax_list = []\nmin_list = []\nprint('start population')\n\npop = toolbox.population(n=1000)\nprint('Population created')\n\nfor g in tqdm(gen, desc=\"Generations\"):\n    if g%25 == 0:\n        print('Generation ' + str(g))\n    \n\n    all_metrics = np.zeros((len(pop), 3))\n    \n    for i, ind in enumerate(pop):\n        X_train, y_train = df[features].values, df[label].values\n        \n        metrics = np.array(toolbox.evaluate(ind, X_train, y_train))\n        all_metrics[i, :] = metrics\n#     print('Evaluation Done')\n    \n\n    avg_metrics = np.mean(all_metrics, axis=0)\n    \n    for ind in pop:\n        ind.fitness.values = tuple(avg_metrics)\n\n\n    offspring = toolbox.select(pop, len(pop))\n    offspring = list(map(toolbox.clone, offspring))\n\n    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n        if random.random() < 0.5:\n            toolbox.mate(child1, child2)\n            del child1.fitness.values\n            del child2.fitness.values\n\n    for mutant in offspring:\n        if random.random() < 0.3:\n            toolbox.mutate(mutant)\n            del mutant.fitness.values\n\n\n\n    for mutant in offspring:\n        X_mutant, y_mutant = df[features].values, df[label].values\n        mutant.fitness.values = toolbox.evaluate(mutant, X_mutant, y_mutant)\n    \n\n    pop[:] = offspring\n\n\n    fits = [ind.fitness.values for ind in pop]\n\n    length = len(pop)\n    sum2 = sum(np.array(fits)**2)\n    std = np.sqrt(abs(sum2 / length - np.array(avg_metrics)**2))\n    g_max = max(fits, key=lambda x: x[2])  # Maximize R-squared\n    g_min = min(fits, key=lambda x: x[0])  # Minimize RMSE\n\n    max_list.append(g_max)\n    min_list.append(g_min)\n    avg_list.append(avg_metrics)\n\n\n\nbest_ind = tools.selBest(pop, 1)[0]\nprint(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n\n\nfunc = gp.compile(expr=best_ind, pset=pset)\nprint(func)\npredictions = func(*X_train.T)\n\nplt.figure()\nplt.plot(df.index, predictions, label='Predicted')\nplt.plot(df.index, y_train, label='Actual')\nplt.xlabel('Date')\nplt.ylabel('Label')\nplt.legend()\nplt.title('Best Individual Prediction vs Actual')\nplt.show()\n\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(3, 1, 1)\nplt.plot(gen, [item[0] for item in min_list], label=\"minimum RMSE\")\nplt.plot(gen, [item[0] for item in max_list], label=\"maximum RMSE\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"RMSE\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Evolution of RMSE\")\n\nplt.subplot(3, 1, 2)\nplt.plot(gen, [item[1] for item in min_list], label=\"minimum MAE\")\nplt.plot(gen, [item[1] for item in max_list], label=\"maximum MAE\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"MAE\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Evolution of MAE\")\n\nplt.subplot(3, 1, 3)\nplt.plot(gen, [item[2] for item in min_list], label=\"minimum R-squared\")\nplt.plot(gen, [item[2] for item in max_list], label=\"maximum R-squared\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"R-squared\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Evolution of R-squared\")\n\nplt.tight_layout()\nplt.show()\n\n\npareto_front = tools.sortNondominated(pop, len(pop), first_front_only=True)[0]\n\nsorted_pareto_front = sorted(pareto_front, key=lambda ind: ind.fitness.values[0])\n\ntop_20_individuals = sorted_pareto_front[:20]\n\nfor i, ind in enumerate(top_20_individuals):\n    print(f\"Top {i + 1} Individual: {ind}, Fitness: {ind.fitness.values}\")\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(3, 1, 1)\nplt.plot(gen, [item[0] for item in min_list], label=\"minimum RMSE\")\nplt.plot(gen, [item[0] for item in max_list], label=\"maximum RMSE\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"RMSE\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Evolution of RMSE\")\n\nplt.subplot(3, 1, 2)\nplt.plot(gen, [item[1] for item in min_list], label=\"minimum MAE\")\nplt.plot(gen, [item[1] for item in max_list], label=\"maximum MAE\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"MAE\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Evolution of MAE\")\n\nplt.subplot(3, 1, 3)\nplt.plot(gen, [item[2] for item in min_list], label=\"minimum R-squared\")\nplt.plot(gen, [item[2] for item in max_list], label=\"maximum R-squared\")\nplt.xlabel(\"Generation\")\nplt.ylabel(\"R-squared\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Evolution of R-squared\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T15:22:19.959816Z","iopub.execute_input":"2023-11-30T15:22:19.961181Z","iopub.status.idle":"2023-11-30T15:25:28.814924Z","shell.execute_reply.started":"2023-11-30T15:22:19.961123Z","shell.execute_reply":"2023-11-30T15:25:28.813300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Peaks and Falls\n## Utilizing our Models' Strengths","metadata":{}},{"cell_type":"markdown","source":"### Plot Vaccinations","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(df.index, df[label], marker='o', linestyle='-', color='b')\nplt.title('Weekly Sum of Vaccinations')\nplt.xlabel('Date')\nplt.ylabel('Weekly Sum of Vaccinations')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Determine the best performance ranges for our model \n### Best_idx is populated with train and test indices for further analysis\n### Indices are chosen based on r^2 value > 0.5","metadata":{}},{"cell_type":"code","source":"indexes = []\nfor test_size in range(3,7):\n    for train_end in range(10,55):\n        indexes.append(([i for i in range(0,train_end)],[i for i in range(train_end,train_end+test_size)]))\n    \nbest_idx = []\n\nfor train_index, test_index in tqdm(indexes):\n\n    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n    X_train, X_test = train_data[features], test_data[features]\n    y_train, y_test = train_data[label], test_data[label]\n\n    rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n    rf_regressor.fit(X_train, y_train)\n    rf_predictions = rf_regressor.predict(X_test)\n\n    xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n    xgb_regressor.fit(X_train, y_train)\n    xgb_predictions = xgb_regressor.predict(X_test)\n    \n    ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n    ada_regressor.fit(X_train, y_train)\n    ada_predictions = ada_regressor.predict(X_test)\n    \n    mse_rf = mean_squared_error(y_test, rf_predictions)\n    mae_rf = mean_absolute_error(y_test, rf_predictions)\n    nmae_rf = normalized_mae(y_test, rf_predictions)\n    nrmse_rf = normalized_rmse(y_test, rf_predictions)\n    r2_rf = r2_score(y_test, rf_predictions)\n\n    mse_xgb = mean_squared_error(y_test, xgb_predictions)\n    mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n    nmae_xgb = normalized_mae(y_test, xgb_predictions)\n    nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n    r2_xgb = r2_score(y_test, xgb_predictions)\n\n    mse_ada = mean_squared_error(y_test, ada_predictions)\n    mae_ada = mean_absolute_error(y_test, ada_predictions)\n    nmae_ada = normalized_mae(y_test, ada_predictions)\n    nrmse_ada = normalized_rmse(y_test, ada_predictions)\n    r2_ada = r2_score(y_test, ada_predictions)\n    \n    if r2_rf >= 0.5 or r2_xgb >= 0.5 or r2_ada >= 0.5:\n        best_idx.append((train_index, test_index))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Store results from our best index ranges","metadata":{}},{"cell_type":"code","source":"\nresults_list = []\n\nfor train_index, test_index in best_idx:\n\n    print(train_index, test_index)\n    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n    X_train, X_test = train_data[features], test_data[features]\n    y_train, y_test = train_data[label], test_data[label]\n\n    # Random Forest\n    rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n    rf_regressor.fit(X_train, y_train)\n    rf_predictions = rf_regressor.predict(X_test)\n\n    # XGBoost\n    xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n    xgb_regressor.fit(X_train, y_train)\n    xgb_predictions = xgb_regressor.predict(X_test)\n\n    # AdaBoost\n    ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n    ada_regressor.fit(X_train, y_train)\n    ada_predictions = ada_regressor.predict(X_test)\n\n    r2_rf = r2_score(y_test, rf_predictions)\n    r2_xgb = r2_score(y_test, xgb_predictions)\n    r2_ada = r2_score(y_test, ada_predictions)\n\n    results_list.append((train_index, test_index, r2_rf, 'Random Forest'))\n    results_list.append((train_index, test_index, r2_xgb, 'XGBoost'))\n    results_list.append((train_index, test_index, r2_ada, 'AdaBoost'))\n\nresults_list.sort(key=lambda x: x[2], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print Results and Best models for those ranges","metadata":{}},{"cell_type":"code","source":"\nfor result in results_list:\n    if result[2] >= 0.5:\n        print(f\"Train Index: {result[0]}, Test Index: {result[1]}, R-squared: {result[2]}, Model Type: {result[3]}\")\n        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output Curves for Best Models on Best Ranges","metadata":{}},{"cell_type":"code","source":"# Filter through results ranges for presentation\nfinal_indexes = results_list[:8]\n\nfor train_index, test_index, _, model_type in final_indexes:\n\n    print(train_index, test_index)\n    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n    X_train, X_test = train_data[features], test_data[features]\n    y_train, y_test = train_data[label], test_data[label]\n    \n    if model_type == 'Random Forest':\n\n        rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n        rf_regressor.fit(X_train, y_train)\n        rf_predictions = rf_regressor.predict(X_test)\n\n        mse_rf = mean_squared_error(y_test, rf_predictions)\n        mae_rf = mean_absolute_error(y_test, rf_predictions)\n        nmae_rf = normalized_mae(y_test, rf_predictions)\n        nrmse_rf = normalized_rmse(y_test, rf_predictions)\n        r2_rf = r2_score(y_test, rf_predictions)\n    \n        print(\"\\nRandom Forest Metrics:\")\n        print(f\"Mean Squared Error: {mse_rf}\")\n        print(f\"Mean Absolute Error: {mae_rf}\")\n        print(f\"Normalized RMSE: {nrmse_rf}\")\n        print(f\"Normalized MAE: {nmae_rf}\")\n        print(f\"R-squared: {r2_rf}\")\n    \n    if model_type == 'XGBoost':\n    \n        xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n        xgb_regressor.fit(X_train, y_train)\n        xgb_predictions = xgb_regressor.predict(X_test)\n\n        mse_xgb = mean_squared_error(y_test, xgb_predictions)\n        mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n        nmae_xgb = normalized_mae(y_test, xgb_predictions)\n        nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n        r2_xgb = r2_score(y_test, xgb_predictions)\n\n        print(\"\\nXGBoost Metrics:\")\n        print(f\"Mean Squared Error: {mse_xgb}\")\n        print(f\"Mean Absolute Error: {mae_xgb}\")\n        print(f\"Normalized RMSE: {nrmse_xgb}\")\n        print(f\"Normalized MAE: {nmae_xgb}\")\n        print(f\"R-squared: {r2_xgb}\")\n    \n    if model_type == 'AdaBoost':\n        ada_regressor = AdaBoostRegressor(n_estimators=500, random_state=42)\n        ada_regressor.fit(X_train, y_train)\n        ada_predictions = ada_regressor.predict(X_test)\n\n        mse_ada = mean_squared_error(y_test, ada_predictions)\n        mae_ada = mean_absolute_error(y_test, ada_predictions)\n        nmae_ada = normalized_mae(y_test, ada_predictions)\n        nrmse_ada = normalized_rmse(y_test, ada_predictions)\n        r2_ada = r2_score(y_test, ada_predictions)\n\n        print(\"\\nAdaBoost Metrics:\")\n        print(f\"Mean Squared Error: {mse_ada}\")\n        print(f\"Mean Absolute Error: {mae_ada}\")\n        print(f\"Normalized RMSE: {nrmse_ada}\")\n        print(f\"Normalized MAE: {nmae_ada}\")\n        print(f\"R-squared: {r2_ada}\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(test_data.index, y_test, label='Actual', color='blue')\n    if model_type == 'Random Forest':\n        plt.plot(test_data.index, rf_predictions, label='Random Forest', color='red')\n    if model_type == 'XGBoost':\n        plt.plot(test_data.index, xgb_predictions, label='XGBoost', color='green')\n    if model_type == 'AdaBoost':\n        plt.plot(test_data.index, ada_predictions, label='AdaBoost', color='purple')\n    plt.xlabel('Index')\n    plt.ylabel('Label')\n    plt.legend()\n    plt.title(f'Regression Curves')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpretability with SHAP\n## Calculates shapley values for individual models on their best ranges and outputs force plots for each week","metadata":{}},{"cell_type":"code","source":"shap_values_dict = {}\n\ni = 0\n\nfor train_index, test_index, _, model_type in final_indexes:\n    print(train_index, test_index)\n\n    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n    X_train, X_test = train_data[features], test_data[features]\n    y_train, y_test = train_data[label], test_data[label]\n\n    if model_type == 'Random Forest':\n        rf_regressor = RandomForestRegressor(n_estimators=500, random_state=42)\n        rf_regressor.fit(X_train, y_train)\n        rf_predictions = rf_regressor.predict(X_test)\n\n        mse_rf = mean_squared_error(y_test, rf_predictions)\n        mae_rf = mean_absolute_error(y_test, rf_predictions)\n        nmae_rf = normalized_mae(y_test, rf_predictions)\n        nrmse_rf = normalized_rmse(y_test, rf_predictions)\n        r2_rf = r2_score(y_test, rf_predictions)\n\n        print(\"\\nRandom Forest Metrics:\")\n        print(f\"Mean Squared Error: {mse_rf}\")\n        print(f\"Mean Absolute Error: {mae_rf}\")\n        print(f\"Normalized RMSE: {nrmse_rf}\")\n        print(f\"Normalized MAE: {nmae_rf}\")\n        print(f\"R-squared: {r2_rf}\")\n\n        explainer_rf = shap.TreeExplainer(rf_regressor)\n        shap_values_rf = explainer_rf.shap_values(X_test)\n\n        shap_values_dict[i] = shap_values_rf\n\n        for j in range(len(test_index)):\n            force_plot = shap.force_plot(explainer_rf.expected_value, shap_values_rf[j, :], X_test.iloc[j, :], feature_names=features)\n            display(HTML(force_plot.html()))\n            \n    if model_type == 'XGBoost':\n        xgb_regressor = XGBRegressor(n_estimators=500, random_state=42)\n        xgb_regressor.fit(X_train, y_train)\n        xgb_predictions = xgb_regressor.predict(X_test)\n\n        mse_xgb = mean_squared_error(y_test, xgb_predictions)\n        mae_xgb = mean_absolute_error(y_test, xgb_predictions)\n        nmae_xgb = normalized_mae(y_test, xgb_predictions)\n        nrmse_xgb = normalized_rmse(y_test, xgb_predictions)\n        r2_xgb = r2_score(y_test, xgb_predictions)\n\n        print(\"\\nXGBoost Metrics:\")\n        print(f\"Mean Squared Error: {mse_xgb}\")\n        print(f\"Mean Absolute Error: {mae_xgb}\")\n        print(f\"Normalized RMSE: {nrmse_xgb}\")\n        print(f\"Normalized MAE: {nmae_xgb}\")\n        print(f\"R-squared: {r2_xgb}\")\n\n        explainer_xgb = shap.Explainer(xgb_regressor)\n        shap_values_xgb = explainer_xgb.shap_values(X_test)\n\n        shap_values_dict[i] = shap_values_xgb\n\n        for j in range(len(test_index)):\n            force_plot = shap.force_plot(explainer_xgb.expected_value, shap_values_xgb[j, :], X_test.iloc[j, :], feature_names=features)\n            display(HTML(force_plot.html()))\n    i += 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
